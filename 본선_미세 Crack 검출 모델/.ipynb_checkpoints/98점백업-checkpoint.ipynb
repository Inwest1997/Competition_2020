{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##태스크 번호 153: 미세 Crack 검출 모델 [태영세라믹]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "print(\"--sys.version—\")\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Input\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dropout, Activation, Average\n",
    "from keras.utils import to_categorical\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.optimizers import Adam\n",
    "from keras.datasets import cifar10\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(2)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.utils.np_utils import to_categorical \n",
    "from keras import Sequential\n",
    "from keras.layers import Conv2D, Flatten, Dense,Dropout,MaxPool2D, Activation,BatchNormalization\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "sns.set(style='white', context='notebook', palette='deep')\n",
    "\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from keras.layers import Dense, Input, Activation\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers.normalization import BatchNormalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path_dir = \"/home/workspace/data/.train/.task153/data/train\"\n",
    "file_list = os.listdir(path_dir)\n",
    "# print(len(file_list))\n",
    "# print(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data = np.loadtxt(\"/home/workspace/data/.train/.task153/data/train/train.csv\",delimiter=\",\",dtype='str')\n",
    "train_df = pd.DataFrame(csv_data[1:], columns=['file_name', 'tile_name', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image as pilimg\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "path_dir = '/home/workspace/data/.train/.task153/data/train/'\n",
    "# file_list = os.listdir(path_dir)\n",
    "pix=[]\n",
    "for i in range(len(train_df)) :\n",
    "    im = pilimg.open( path_dir + train_df['file_name'][i] )\n",
    "    im = np.array(im)\n",
    "    pix.append(im)\n",
    "    #im = im.resize((224, 224))\n",
    "    # print(im.size)\n",
    "\n",
    "    #pix = np.r_[[pix],[np.array(im)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# file_list = os.listdir(path_dir)\n",
    "for i in range(len(train_df)):\n",
    "    \n",
    "    if(train_df['label'][i]=='1'): # 크랙이 존재하는 데이터\n",
    "        im = pilimg.open( path_dir + train_df['file_name'][i] )\n",
    "                \n",
    "        x = img_to_array(im)  # (3, 150, 150) 크기의 NumPy 배열\n",
    "        x = x.reshape((1,) + x.shape)  # (1, 3, 150, 150) 크기의 NumPy 배열\n",
    "    \n",
    "        i = 0\n",
    "        for batch in datagen.flow(x, batch_size=1):\n",
    "            \n",
    "            # 차원 축소\n",
    "            batch = np.squeeze(batch)\n",
    "            \n",
    "            if i != 0:\n",
    "                batch = np.array(batch)\n",
    "                pix.append(batch)\n",
    "            \n",
    "            i += 1\n",
    "            if i > 7:\n",
    "                break  # 이미지 10장을 생성하고 마칩니다\n",
    "    #im = im.resize((224, 224))\n",
    "    # print(im.size)\n",
    "\n",
    "    #pix = np.r_[[pix],[np.array(im)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "crack_df= train_df[train_df['label']=='1']\n",
    "count=0\n",
    "fig = plt.figure(figsize=(50,50))\n",
    "rows = 10\n",
    "cols = 5\n",
    "for crack in crack_df['file_name']:\n",
    "    if count==0:\n",
    "        count+=1\n",
    "        continue\n",
    "        \n",
    "    elif count==31:\n",
    "        break\n",
    "    else:\n",
    "        img = pilimg.open( path_dir + crack )\n",
    "        ax = fig.add_subplot(rows, cols, count)\n",
    "        ax.imshow(img)\n",
    "#         ax.set_xlabel(xlabels[count])\n",
    "        ax.set_xticks([]), ax.set_yticks([])\n",
    "        count+=1\n",
    "plt.show()\n",
    "# crack_img\n",
    "# crack_df['file_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y= np.array(train_df['label'])\n",
    "# for i in range(10458):\n",
    "#     y = np.append(y, '1')\n",
    "while len(y)<len(pix):\n",
    "    y = np.append(y, '1')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "g = sns.countplot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pix=np.array(pix)\n",
    "pix=pix.astype('int32')\n",
    "y=y.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k=np.array(pix)\n",
    "# q=np.array(y)\n",
    "print(pix.shape)\n",
    "print(y.shape)\n",
    "# pix[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "(24531, 68, 68, 3)\n",
    "(24531,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(pix, y, test_size = 0.1, random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = shape=(68, 68, 3)\n",
    "model_input = Input(shape=input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=100\n",
    "EPOCH=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # With data augmentation to prevent overfitting (accuracy 0.99286)\n",
    "\n",
    "# datagen = ImageDataGenerator(\n",
    "#         rescale = 1./255,\n",
    "#         featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "#         samplewise_center=False,  # set each sample mean to 0\n",
    "#         featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "#         samplewise_std_normalization=False,  # divide each input by its std\n",
    "#         zca_whitening=False,  # apply ZCA whitening\n",
    "#         rotation_range=30,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "#         zoom_range = 0, # Randomly zoom image \n",
    "#         width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "#         height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "#         horizontal_flip=True,  # randomly flip images\n",
    "#         vertical_flip=True)  # randomly flip images\n",
    "\n",
    "\n",
    "# datagen.fit(pix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', \n",
    "                                            patience=3, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_and_train(model, num_epochs): \n",
    "    \n",
    "    \n",
    "    filepath = '/home/workspace/weights/' + model.name #+ '.hdf5'\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', \n",
    "                                 save_weights_only=True, \n",
    "                                 save_best_only=True,\n",
    "                                 mode='max')#, period=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     tensor_board = TensorBoard(log_dir='logs/', histogram_freq=0, batch_size=32)                              \n",
    "    history = model.fit(x=X_train, y=Y_train, batch_size=BATCH_SIZE, epochs=num_epochs, verbose=2, callbacks=[checkpoint,learning_rate_reduction], validation_split=0.2)\n",
    "    # Fit the model\n",
    "#     history = model.fit_generator(datagen.flow(X_train,Y_train, batch_size=BATCH_SIZE),\n",
    "#                               epochs = num_epochs, \n",
    "#                               validation_data = (X_val,Y_val),\n",
    "#                               verbose = 2,\n",
    "#                              #steps_per_epoch=X_train.shape[0] // BATCH_SIZE,\n",
    "#                               callbacks=[checkpoint,learning_rate_reduction])\n",
    "# #                               validation_split=0.2)\n",
    "                                \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    fig, ax = plt.subplots(2,1)\n",
    "    ax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\n",
    "    ax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\n",
    "    legend = ax[0].legend(loc='best', shadow=True)\n",
    "\n",
    "    ax[1].plot(history.history['accuracy'], color='b', label=\"Training accuracy\")\n",
    "    ax[1].plot(history.history['val_accuracy'], color='r',label=\"Validation accuracy\")\n",
    "    legend = ax[1].legend(loc='best', shadow=True)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "def evaluate_error(model):\n",
    "#     pred = model.predict(X_val, batch_size = 25)\n",
    "#     pred = np.argmax(pred, axis=1)\n",
    "#     pred = np.expand_dims(pred, axis=1) # make same shape as y_test\n",
    "#     error = np.sum(np.not_equal(pred, Y_val)) / Y_val.shape[0]    \n",
    "#     return error\n",
    "\n",
    "\n",
    "#     pred = model.predict(X_val)#, batch_size = batch_size)\n",
    "#     Y_pred_classes = np.argmax(pred,axis = 1) \n",
    "#     report = classification_report(Y_val, Y_pred_classes)\n",
    "#     print(report)\n",
    "\n",
    "    score = model.evaluate(X_val, Y_val, batch_size=BATCH_SIZE,\n",
    "                                            verbose=2 )\n",
    "    print(score)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def recall(y_target, y_pred):\n",
    "    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\n",
    "    # round : 반올림한다\n",
    "    y_target_yn = K.round(K.clip(y_target, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "    y_pred_yn = K.round(K.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "\n",
    "    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
    "    count_true_positive = K.sum(y_target_yn * y_pred_yn) \n",
    "\n",
    "    # (True Positive + False Negative) = 실제 값이 1(Positive) 전체\n",
    "    count_true_positive_false_negative = K.sum(y_target_yn)\n",
    "\n",
    "    # Recall =  (True Positive) / (True Positive + False Negative)\n",
    "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
    "    recall = count_true_positive / (count_true_positive_false_negative + K.epsilon())\n",
    "\n",
    "    # return a single tensor value\n",
    "    return recall\n",
    "\n",
    "\n",
    "def precision(y_target, y_pred):\n",
    "    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\n",
    "    # round : 반올림한다\n",
    "    y_pred_yn = K.round(K.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "    y_target_yn = K.round(K.clip(y_target, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "\n",
    "    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
    "    count_true_positive = K.sum(y_target_yn * y_pred_yn) \n",
    "\n",
    "    # (True Positive + False Positive) = 예측 값이 1(Positive) 전체\n",
    "    count_true_positive_false_positive = K.sum(y_pred_yn)\n",
    "\n",
    "    # Precision = (True Positive) / (True Positive + False Positive)\n",
    "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
    "    precision = count_true_positive / (count_true_positive_false_positive + K.epsilon())\n",
    "\n",
    "    # return a single tensor value\n",
    "    return precision\n",
    "\n",
    "\n",
    "def f1score(y_target, y_pred):\n",
    "    _recall = recall(y_target, y_pred)\n",
    "    _precision = precision(y_target, y_pred)\n",
    "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
    "    _f1score = ( 2 * _recall * _precision) / (_recall + _precision+ K.epsilon())\n",
    "    \n",
    "    # return a single tensor value\n",
    "    return _f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first Model\n",
    "def VGG_model(model_input):\n",
    "    \n",
    "    model = tf.keras.applications.VGG16(\n",
    "        include_top=False,\n",
    "        weights=\"imagenet\",\n",
    "        input_tensor=model_input,\n",
    "        input_shape=(68, 68, 3),\n",
    "        pooling='max',\n",
    "        classes=1,\n",
    "    #     classifier_activation=\"softmax\",|\n",
    "    )\n",
    "\n",
    "    x = model.output\n",
    "    x = Dense(1024, name='fully_VGG', kernel_initializer='random_uniform')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dense(512, kernel_initializer='random_uniform')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dense(1, activation='sigmoid', name='sigmoid_VGG')(x)\n",
    "    model = Model(model_input, x, name = 'VGG_model')\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=Adam(), metrics=['accuracy']) \n",
    "    return model\n",
    "\n",
    "\n",
    "vgg_model = VGG_model(model_input)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
    "58892288/58889256 [==============================] - 11s 0us/step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second Model\n",
    "def ResNet_model(model_input):\n",
    "    \n",
    "    model = ResNet50(input_tensor=model_input, \n",
    "                     include_top=False, \n",
    "                     weights=None, \n",
    "                     pooling='max')\n",
    "\n",
    "    x = model.output\n",
    "    x = Dense(1024, name='fully_ResNet', kernel_initializer='random_uniform')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dense(512, kernel_initializer='random_uniform')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dense(1, activation='sigmoid', name='sigmoid_ResNet')(x)\n",
    "    model = Model(model_input, x, name='ResNet_model')\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=Adam(), metrics=['accuracy']) \n",
    "\n",
    "    return model\n",
    "\n",
    "resnet_model = ResNet_model(model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # third Model\n",
    "# def DenseNet201_model(model_input):\n",
    "#     model = tf.keras.applications.DenseNet201(\n",
    "#     include_top=False,\n",
    "#     weights=\"imagenet\",\n",
    "#     input_tensor=model_input,\n",
    "#     input_shape=(68, 68, 3),\n",
    "#     pooling='max',\n",
    "#     classes=1\n",
    "#     )\n",
    "#     x = model.output\n",
    "#     x = Dense(1024, name='fully_DenseNet201', kernel_initializer='random_uniform')(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = Dense(512, kernel_initializer='random_uniform')(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = Dense(1, activation='sigmoid', name='sigmoid_DenseNet201')(x)\n",
    "#     model = Model(model_input, x, name = 'DenseNet201_model')\n",
    "#     model.compile(loss=\"binary_crossentropy\", optimizer=Adam(), metrics=['accuracy']) \n",
    "    \n",
    "#     return model\n",
    "\n",
    "# denseNet201_model = DenseNet201_model(model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history= compile_and_train(vgg_model, num_epochs=EPOCH)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Epoch 1/50\n",
    "177/177 - 74s - loss: 0.2317 - accuracy: 0.9111 - val_loss: 0.1301 - val_accuracy: 0.9527\n",
    "Epoch 2/50\n",
    "177/177 - 19s - loss: 0.1134 - accuracy: 0.9616 - val_loss: 0.1555 - val_accuracy: 0.9429\n",
    "Epoch 3/50\n",
    "177/177 - 20s - loss: 0.0971 - accuracy: 0.9661 - val_loss: 0.0897 - val_accuracy: 0.9699\n",
    "Epoch 4/50\n",
    "177/177 - 21s - loss: 0.0824 - accuracy: 0.9712 - val_loss: 0.0842 - val_accuracy: 0.9701\n",
    "Epoch 5/50\n",
    "177/177 - 21s - loss: 0.0779 - accuracy: 0.9737 - val_loss: 0.0708 - val_accuracy: 0.9751\n",
    "Epoch 6/50\n",
    "177/177 - 20s - loss: 0.0695 - accuracy: 0.9760 - val_loss: 0.0654 - val_accuracy: 0.9771\n",
    "Epoch 7/50\n",
    "177/177 - 19s - loss: 0.0610 - accuracy: 0.9783 - val_loss: 0.0812 - val_accuracy: 0.9724\n",
    "Epoch 8/50\n",
    "177/177 - 19s - loss: 0.0577 - accuracy: 0.9801 - val_loss: 0.1188 - val_accuracy: 0.9631\n",
    "Epoch 9/50\n",
    "\n",
    "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
    "177/177 - 19s - loss: 0.0514 - accuracy: 0.9817 - val_loss: 0.0898 - val_accuracy: 0.9690\n",
    "Epoch 10/50\n",
    "177/177 - 21s - loss: 0.0323 - accuracy: 0.9890 - val_loss: 0.0540 - val_accuracy: 0.9803\n",
    "Epoch 11/50\n",
    "177/177 - 21s - loss: 0.0264 - accuracy: 0.9898 - val_loss: 0.0490 - val_accuracy: 0.9830\n",
    "Epoch 12/50\n",
    "177/177 - 19s - loss: 0.0256 - accuracy: 0.9905 - val_loss: 0.1040 - val_accuracy: 0.9667\n",
    "Epoch 13/50\n",
    "177/177 - 19s - loss: 0.0218 - accuracy: 0.9914 - val_loss: 0.1860 - val_accuracy: 0.9454\n",
    "Epoch 14/50\n",
    "\n",
    "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
    "177/177 - 19s - loss: 0.0220 - accuracy: 0.9919 - val_loss: 0.0895 - val_accuracy: 0.9746\n",
    "Epoch 15/50\n",
    "177/177 - 21s - loss: 0.0139 - accuracy: 0.9950 - val_loss: 0.0541 - val_accuracy: 0.9853\n",
    "Epoch 16/50\n",
    "177/177 - 19s - loss: 0.0086 - accuracy: 0.9967 - val_loss: 0.0679 - val_accuracy: 0.9812\n",
    "Epoch 17/50\n",
    "177/177 - 19s - loss: 0.0087 - accuracy: 0.9967 - val_loss: 0.0812 - val_accuracy: 0.9787\n",
    "Epoch 18/50\n",
    "\n",
    "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
    "177/177 - 19s - loss: 0.0081 - accuracy: 0.9972 - val_loss: 0.1065 - val_accuracy: 0.9805\n",
    "Epoch 19/50\n",
    "177/177 - 19s - loss: 0.0061 - accuracy: 0.9978 - val_loss: 0.0664 - val_accuracy: 0.9839\n",
    "Epoch 20/50\n",
    "177/177 - 21s - loss: 0.0028 - accuracy: 0.9990 - val_loss: 0.0629 - val_accuracy: 0.9855\n",
    "Epoch 21/50\n",
    "177/177 - 19s - loss: 0.0026 - accuracy: 0.9992 - val_loss: 0.0753 - val_accuracy: 0.9844\n",
    "Epoch 22/50\n",
    "177/177 - 19s - loss: 0.0042 - accuracy: 0.9984 - val_loss: 0.0722 - val_accuracy: 0.9846\n",
    "Epoch 23/50\n",
    "177/177 - 21s - loss: 0.0027 - accuracy: 0.9992 - val_loss: 0.0702 - val_accuracy: 0.9857\n",
    "Epoch 24/50\n",
    "177/177 - 19s - loss: 0.0028 - accuracy: 0.9993 - val_loss: 0.0723 - val_accuracy: 0.9855\n",
    "Epoch 25/50\n",
    "177/177 - 19s - loss: 0.0022 - accuracy: 0.9994 - val_loss: 0.0710 - val_accuracy: 0.9855\n",
    "Epoch 26/50\n",
    "\n",
    "Epoch 00026: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
    "177/177 - 19s - loss: 0.0016 - accuracy: 0.9994 - val_loss: 0.0779 - val_accuracy: 0.9851\n",
    "Epoch 27/50\n",
    "177/177 - 21s - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.0757 - val_accuracy: 0.9871\n",
    "Epoch 28/50\n",
    "177/177 - 19s - loss: 0.0012 - accuracy: 0.9996 - val_loss: 0.0804 - val_accuracy: 0.9855\n",
    "Epoch 29/50\n",
    "177/177 - 19s - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.0765 - val_accuracy: 0.9866\n",
    "Epoch 30/50\n",
    "\n",
    "Epoch 00030: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
    "177/177 - 19s - loss: 0.0012 - accuracy: 0.9996 - val_loss: 0.0806 - val_accuracy: 0.9853\n",
    "Epoch 31/50\n",
    "177/177 - 19s - loss: 9.6119e-04 - accuracy: 0.9998 - val_loss: 0.0807 - val_accuracy: 0.9855\n",
    "Epoch 32/50\n",
    "177/177 - 19s - loss: 5.7991e-04 - accuracy: 0.9998 - val_loss: 0.0790 - val_accuracy: 0.9855\n",
    "Epoch 33/50\n",
    "\n",
    "Epoch 00033: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
    "177/177 - 19s - loss: 7.4741e-04 - accuracy: 0.9998 - val_loss: 0.0800 - val_accuracy: 0.9846\n",
    "Epoch 34/50\n",
    "177/177 - 19s - loss: 5.3406e-04 - accuracy: 0.9998 - val_loss: 0.0810 - val_accuracy: 0.9855\n",
    "Epoch 35/50\n",
    "177/177 - 19s - loss: 4.6462e-04 - accuracy: 0.9998 - val_loss: 0.0840 - val_accuracy: 0.9853\n",
    "Epoch 36/50\n",
    "\n",
    "Epoch 00036: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
    "177/177 - 19s - loss: 8.1989e-04 - accuracy: 0.9997 - val_loss: 0.0821 - val_accuracy: 0.9855\n",
    "Epoch 37/50\n",
    "177/177 - 19s - loss: 6.0244e-04 - accuracy: 0.9999 - val_loss: 0.0816 - val_accuracy: 0.9855\n",
    "Epoch 38/50\n",
    "177/177 - 19s - loss: 4.9461e-04 - accuracy: 0.9999 - val_loss: 0.0828 - val_accuracy: 0.9853\n",
    "Epoch 39/50\n",
    "177/177 - 19s - loss: 3.7551e-04 - accuracy: 0.9999 - val_loss: 0.0824 - val_accuracy: 0.9851\n",
    "Epoch 40/50\n",
    "177/177 - 19s - loss: 6.3918e-04 - accuracy: 0.9998 - val_loss: 0.0825 - val_accuracy: 0.9851\n",
    "Epoch 41/50\n",
    "177/177 - 19s - loss: 5.8220e-04 - accuracy: 0.9997 - val_loss: 0.0824 - val_accuracy: 0.9855\n",
    "Epoch 42/50\n",
    "177/177 - 19s - loss: 4.4743e-04 - accuracy: 0.9998 - val_loss: 0.0821 - val_accuracy: 0.9853\n",
    "Epoch 43/50\n",
    "177/177 - 19s - loss: 6.1446e-04 - accuracy: 0.9997 - val_loss: 0.0826 - val_accuracy: 0.9855\n",
    "Epoch 44/50\n",
    "177/177 - 19s - loss: 6.8648e-04 - accuracy: 0.9998 - val_loss: 0.0826 - val_accuracy: 0.9855\n",
    "Epoch 45/50\n",
    "177/177 - 19s - loss: 3.3418e-04 - accuracy: 0.9999 - val_loss: 0.0835 - val_accuracy: 0.9855\n",
    "Epoch 46/50\n",
    "177/177 - 19s - loss: 2.6210e-04 - accuracy: 1.0000 - val_loss: 0.0852 - val_accuracy: 0.9855\n",
    "Epoch 47/50\n",
    "177/177 - 19s - loss: 3.8933e-04 - accuracy: 0.9999 - val_loss: 0.0845 - val_accuracy: 0.9855\n",
    "Epoch 48/50\n",
    "177/177 - 19s - loss: 5.0707e-04 - accuracy: 0.9998 - val_loss: 0.0861 - val_accuracy: 0.9855\n",
    "Epoch 49/50\n",
    "177/177 - 17s - loss: 6.8043e-04 - accuracy: 0.9997 - val_loss: 0.0848 - val_accuracy: 0.9857\n",
    "Epoch 50/50\n",
    "177/177 - 17s - loss: 5.8702e-04 - accuracy: 0.9998 - val_loss: 0.0872 - val_accuracy: 0.9855\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = compile_and_train(resnet_model, num_epochs=EPOCH)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Epoch 1/50\n",
    "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
    "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
    "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
    "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
    "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
    "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
    "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
    "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
    "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
    "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
    "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
    "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
    "212/212 - 68s - loss: 0.3007 - accuracy: 0.8731 - val_loss: 0.5310 - val_accuracy: 0.7341\n",
    "Epoch 2/50\n",
    "212/212 - 25s - loss: 0.1412 - accuracy: 0.9480 - val_loss: 0.6563 - val_accuracy: 0.8178\n",
    "Epoch 3/50\n",
    "212/212 - 23s - loss: 0.1100 - accuracy: 0.9579 - val_loss: 1.5208 - val_accuracy: 0.7105\n",
    "Epoch 4/50\n",
    "212/212 - 25s - loss: 0.0996 - accuracy: 0.9601 - val_loss: 0.8566 - val_accuracy: 0.8262\n",
    "Epoch 5/50\n",
    "212/212 - 23s - loss: 0.0849 - accuracy: 0.9675 - val_loss: 1.6691 - val_accuracy: 0.7496\n",
    "Epoch 6/50\n",
    "212/212 - 23s - loss: 0.0763 - accuracy: 0.9705 - val_loss: 1.4741 - val_accuracy: 0.6548\n",
    "Epoch 7/50\n",
    "\n",
    "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
    "212/212 - 23s - loss: 0.0718 - accuracy: 0.9726 - val_loss: 1.1962 - val_accuracy: 0.7978\n",
    "Epoch 8/50\n",
    "212/212 - 23s - loss: 0.0440 - accuracy: 0.9838 - val_loss: 1.6815 - val_accuracy: 0.7453\n",
    "Epoch 9/50\n",
    "212/212 - 23s - loss: 0.0360 - accuracy: 0.9860 - val_loss: 0.9747 - val_accuracy: 0.8137\n",
    "Epoch 10/50\n",
    "212/212 - 25s - loss: 0.0315 - accuracy: 0.9885 - val_loss: 0.3545 - val_accuracy: 0.9114\n",
    "Epoch 11/50\n",
    "212/212 - 23s - loss: 0.0317 - accuracy: 0.9889 - val_loss: 1.7799 - val_accuracy: 0.7543\n",
    "Epoch 12/50\n",
    "212/212 - 23s - loss: 0.0374 - accuracy: 0.9862 - val_loss: 0.6138 - val_accuracy: 0.8751\n",
    "Epoch 13/50\n",
    "\n",
    "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
    "212/212 - 23s - loss: 0.0364 - accuracy: 0.9861 - val_loss: 1.0935 - val_accuracy: 0.7974\n",
    "Epoch 14/50\n",
    "212/212 - 25s - loss: 0.0168 - accuracy: 0.9941 - val_loss: 0.1087 - val_accuracy: 0.9666\n",
    "Epoch 15/50\n",
    "212/212 - 23s - loss: 0.0123 - accuracy: 0.9958 - val_loss: 1.6343 - val_accuracy: 0.7725\n",
    "Epoch 16/50\n",
    "212/212 - 25s - loss: 0.0152 - accuracy: 0.9948 - val_loss: 0.1169 - val_accuracy: 0.9700\n",
    "Epoch 17/50\n",
    "212/212 - 23s - loss: 0.0118 - accuracy: 0.9963 - val_loss: 0.3358 - val_accuracy: 0.9250\n",
    "Epoch 18/50\n",
    "212/212 - 23s - loss: 0.0136 - accuracy: 0.9952 - val_loss: 0.8497 - val_accuracy: 0.8796\n",
    "Epoch 19/50\n",
    "\n",
    "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
    "212/212 - 23s - loss: 0.0126 - accuracy: 0.9951 - val_loss: 0.1352 - val_accuracy: 0.9681\n",
    "Epoch 20/50\n",
    "212/212 - 23s - loss: 0.0043 - accuracy: 0.9987 - val_loss: 0.1682 - val_accuracy: 0.9645\n",
    "Epoch 21/50\n",
    "212/212 - 23s - loss: 0.0035 - accuracy: 0.9991 - val_loss: 0.2250 - val_accuracy: 0.9577\n",
    "Epoch 22/50\n",
    "\n",
    "Epoch 00022: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
    "212/212 - 23s - loss: 0.0030 - accuracy: 0.9991 - val_loss: 0.1789 - val_accuracy: 0.9654\n",
    "Epoch 23/50\n",
    "212/212 - 25s - loss: 0.0017 - accuracy: 0.9994 - val_loss: 0.1051 - val_accuracy: 0.9786\n",
    "Epoch 24/50\n",
    "212/212 - 25s - loss: 0.0014 - accuracy: 0.9996 - val_loss: 0.1093 - val_accuracy: 0.9794\n",
    "Epoch 25/50\n",
    "212/212 - 25s - loss: 7.4166e-04 - accuracy: 0.9997 - val_loss: 0.1080 - val_accuracy: 0.9798\n",
    "Epoch 26/50\n",
    "212/212 - 25s - loss: 8.9869e-04 - accuracy: 0.9997 - val_loss: 0.1031 - val_accuracy: 0.9809\n",
    "Epoch 27/50\n",
    "212/212 - 23s - loss: 0.0026 - accuracy: 0.9991 - val_loss: 0.0942 - val_accuracy: 0.9794\n",
    "Epoch 28/50\n",
    "212/212 - 23s - loss: 6.4635e-04 - accuracy: 0.9998 - val_loss: 0.1369 - val_accuracy: 0.9747\n",
    "Epoch 29/50\n",
    "\n",
    "Epoch 00029: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
    "212/212 - 23s - loss: 3.8568e-04 - accuracy: 0.9998 - val_loss: 0.1238 - val_accuracy: 0.9779\n",
    "Epoch 30/50\n",
    "212/212 - 23s - loss: 6.6141e-04 - accuracy: 0.9998 - val_loss: 0.1215 - val_accuracy: 0.9783\n",
    "Epoch 31/50\n",
    "212/212 - 23s - loss: 3.6310e-04 - accuracy: 0.9998 - val_loss: 0.1292 - val_accuracy: 0.9783\n",
    "Epoch 32/50\n",
    "\n",
    "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
    "212/212 - 23s - loss: 2.2059e-04 - accuracy: 0.9999 - val_loss: 0.1226 - val_accuracy: 0.9792\n",
    "Epoch 33/50\n",
    "212/212 - 23s - loss: 2.6697e-04 - accuracy: 0.9999 - val_loss: 0.1185 - val_accuracy: 0.9792\n",
    "Epoch 34/50\n",
    "212/212 - 23s - loss: 2.6370e-04 - accuracy: 0.9999 - val_loss: 0.1249 - val_accuracy: 0.9798\n",
    "Epoch 35/50\n",
    "\n",
    "Epoch 00035: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
    "212/212 - 23s - loss: 1.9294e-04 - accuracy: 0.9999 - val_loss: 0.1247 - val_accuracy: 0.9796\n",
    "Epoch 36/50\n",
    "212/212 - 23s - loss: 2.2115e-04 - accuracy: 0.9999 - val_loss: 0.1251 - val_accuracy: 0.9798\n",
    "Epoch 37/50\n",
    "212/212 - 23s - loss: 1.8084e-04 - accuracy: 0.9999 - val_loss: 0.1264 - val_accuracy: 0.9800\n",
    "Epoch 38/50\n",
    "212/212 - 23s - loss: 2.0276e-04 - accuracy: 0.9999 - val_loss: 0.1275 - val_accuracy: 0.9796\n",
    "Epoch 39/50\n",
    "212/212 - 23s - loss: 2.0371e-04 - accuracy: 0.9999 - val_loss: 0.1300 - val_accuracy: 0.9792\n",
    "Epoch 40/50\n",
    "212/212 - 23s - loss: 1.7078e-04 - accuracy: 0.9999 - val_loss: 0.1285 - val_accuracy: 0.9798\n",
    "Epoch 41/50\n",
    "212/212 - 23s - loss: 1.8914e-04 - accuracy: 0.9999 - val_loss: 0.1287 - val_accuracy: 0.9796\n",
    "Epoch 42/50\n",
    "212/212 - 23s - loss: 1.8197e-04 - accuracy: 1.0000 - val_loss: 0.1315 - val_accuracy: 0.9796\n",
    "Epoch 43/50\n",
    "212/212 - 23s - loss: 2.2432e-04 - accuracy: 0.9998 - val_loss: 0.1320 - val_accuracy: 0.9796\n",
    "Epoch 44/50\n",
    "212/212 - 23s - loss: 1.7564e-04 - accuracy: 0.9999 - val_loss: 0.1310 - val_accuracy: 0.9794\n",
    "Epoch 45/50\n",
    "212/212 - 23s - loss: 1.9679e-04 - accuracy: 0.9999 - val_loss: 0.1340 - val_accuracy: 0.9792\n",
    "Epoch 46/50\n",
    "212/212 - 23s - loss: 1.5190e-04 - accuracy: 0.9999 - val_loss: 0.1357 - val_accuracy: 0.9786\n",
    "Epoch 47/50\n",
    "212/212 - 23s - loss: 1.5773e-04 - accuracy: 1.0000 - val_loss: 0.1348 - val_accuracy: 0.9794\n",
    "Epoch 48/50\n",
    "212/212 - 23s - loss: 1.9036e-04 - accuracy: 0.9999 - val_loss: 0.1393 - val_accuracy: 0.9792\n",
    "Epoch 49/50\n",
    "212/212 - 23s - loss: 1.4916e-04 - accuracy: 1.0000 - val_loss: 0.1391 - val_accuracy: 0.9794\n",
    "Epoch 50/50\n",
    "212/212 - 23s - loss: 1.9080e-04 - accuracy: 0.9999 - val_loss: 0.1423 - val_accuracy: 0.9788\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = compile_and_train(denseNet201_model, num_epochs=EPOCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_model = VGG_model(model_input)\n",
    "resnet_model = ResNet_model(model_input)\n",
    "# denseNet201_model = DenseNet201_model(model_input)\n",
    "\n",
    "vgg_model.load_weights('/home/workspace/weights/VGG_model')#.hdf5\n",
    "resnet_model.load_weights('/home/workspace/weights/ResNet_model')\n",
    "# denseNet201_model.load_weights('/home/workspace/weights/DenseNet201_model')\n",
    "\n",
    "models = [vgg_model, resnet_model]#,denseNet201_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ensemble(models, model_input):\n",
    "    \n",
    "#     outputs = [model.outputs[0] for model in models]\n",
    "#     y = Average()(outputs)\n",
    "    \n",
    "#     model = Model(model_input, y, name='ensemble2')\n",
    "#     model.compile(loss=\"binary_crossentropy\", optimizer=Adam(), metrics=['accuracy']) \n",
    "    \n",
    "#     return model\n",
    "# ensemble_model = ensemble(models, model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble(models, model_input):\n",
    "    \n",
    "    outputs = [model.outputs[0] for model in models]\n",
    "    y = Average()(outputs)\n",
    "    \n",
    "    model = Model(model_input, y, name='ensemble2')\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=Adam(), metrics=['accuracy']) \n",
    "    \n",
    "    return model\n",
    "ensemble_model = ensemble(models, model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('vgg_model')\n",
    "evaluate_error(vgg_model)\n",
    "print()\n",
    "print()\n",
    "print('resnet_model')\n",
    "evaluate_error(resnet_model)\n",
    "# print()\n",
    "# print()\n",
    "# print('denseNet201_model')\n",
    "# evaluate_error(DenseNet201_model)\n",
    "print()\n",
    "print()\n",
    "print('ensemble_model')\n",
    "evaluate_error(ensemble_model)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "vgg_model\n",
    "30/30 - 1s - loss: 0.0676 - accuracy: 0.9898\n",
    "[0.06758183240890503, 0.9897959232330322]\n",
    "\n",
    "\n",
    "resnet_model\n",
    "30/30 - 1s - loss: 0.1307 - accuracy: 0.9789\n",
    "[0.1307269036769867, 0.9789115786552429]\n",
    "\n",
    "\n",
    "ensemble_model\n",
    "30/30 - 2s - loss: 0.0542 - accuracy: 0.9871\n",
    "[0.05418480560183525, 0.9870748519897461]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at confusion matrix \n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Predict the values from the validation dataset\n",
    "Y_pred = ensemble_model.predict(X_val)\n",
    "# Convert predictions classes to one hot vectors \n",
    "# Y_pred_classes = np.argmax(Y_pred,axis = 1) \n",
    "# Convert validation observations to one hot vectors\n",
    "# Y_true = np.argmax(Y_val,axis = 1) \n",
    "# compute the confusion matrix\n",
    "\n",
    "Y_pred_class= tf.cast(Y_pred>0.5,dtype = tf.float32)\n",
    "confusion_mtx = confusion_matrix(Y_val, Y_pred_class) \n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(confusion_mtx, classes = range(2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
